{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b071b2c",
   "metadata": {},
   "source": [
    "# Notebook 01: Data loading & memory optimization\n",
    "\n",
    "## WHhy does this step matter ?\n",
    "\n",
    "### The Problem\n",
    "\n",
    "The Instacart dataset contains **32+ million order-product pairs**. Loading this data with default pandas data types (int64, float64) consumes approximately **1.5GB of RAM**. This creates several problems:\n",
    "\n",
    "1. **Team collaboration and github compliance**: Not all team members have powerful machines github only allows 100MB max per file\n",
    "2. **Processing speed**: The larger the datasets are, the slower are the operations\n",
    "\n",
    "### The Solution\n",
    "\n",
    "**We optimize memory through data type conversion** without losing any information. This is achieved by:\n",
    "- Converting `int64` → `int32/int16/int8` based on actual value ranges\n",
    "- Converting `float64` → `float32` for decimal values\n",
    "- Converting `object` → `category` for text columns with few unique values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5e6f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NOTEBOOK 01: DATA LOADING & MEMORY OPTIMIZATION\n",
      "======================================================================\n",
      "Execution started: 2026-02-20 13:35:22\n"
     ]
    }
   ],
   "source": [
    "### Step 1: Import Required Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NOTEBOOK 01: DATA LOADING & MEMORY OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Execution started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ebe8e2",
   "metadata": {},
   "source": [
    "**Why these libraries?**\n",
    "- pandas: Data manipulation and analysis\n",
    "-numpy: Numerical operations\n",
    "-os: File path operations\n",
    "-json: Saving metadata for PDF report\n",
    "-datetime: Timestamps for documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "663e6716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: ../data\n",
      "Output directory: ../data/processed\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data'\n",
    "OUTPUT_DIR = '../data/processed'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ef809",
   "metadata": {},
   "source": [
    "We separate paths to keeps raw data separate from processed one and enable reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba5862b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] Loading data...\n",
      "   orders: 3,421,083 rows\n",
      "   orders_prior: 32,434,489 rows\n",
      "   products: 49,688 rows\n",
      "   products priced: 1,000 rows\n",
      "   aisles: 134 rows (134 aisles)\n",
      "   departments: 21 rows (21 departments)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[1/3] Loading data...\")\n",
    "\n",
    "orders = pd.read_csv(f'{DATA_DIR}/orders.csv')\n",
    "print(f\"   orders: {len(orders):,} rows\")\n",
    "\n",
    "orders_prior = pd.read_csv(f'{DATA_DIR}/order_products__prior.csv')\n",
    "print(f\"   orders_prior: {len(orders_prior):,} rows\")\n",
    "\n",
    "products = pd.read_csv(f'{DATA_DIR}/products.csv')\n",
    "print(f\"   products: {len(products):,} rows\")\n",
    "\n",
    "products_priced = pd.read_csv(f'{DATA_DIR}/product_with_price(in).csv')\n",
    "print(f\"   products priced: {len(products_priced):,} rows\")\n",
    "\n",
    "aisles = pd.read_csv(f'{DATA_DIR}/aisles.csv')\n",
    "print(f\"   aisles: {len(aisles)} rows (134 aisles)\")\n",
    "\n",
    "departments = pd.read_csv(f'{DATA_DIR}/departments.csv')\n",
    "print(f\"   departments: {len(departments)} rows (21 departments)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1145c",
   "metadata": {},
   "source": [
    "As all files are needed for our analysis, we load them all at once. Also, this enable us to check data quality before any processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bb57b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/3] Optimizing memory...\n",
      "\n",
      "Optimizing orders...\n",
      "\n",
      "Optimizing orders_prior...\n",
      "\n",
      "Optimizing products...\n",
      "\n",
      "Optimizing products...\n",
      "\n",
      "Optimizing aisles...\n",
      "\n",
      "Optimizing departments...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2/3] Optimizing memory...\")\n",
    "\n",
    "def optimize_dtypes(df, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Reduce memory by converting to smaller data types.\n",
    "    Source: Project2_PythonML_A25.pdf Section 4\n",
    "    \"\"\"\n",
    "    print(f\"\\nOptimizing {name}...\")\n",
    "    memory_before = df.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # Integers\n",
    "        if col_type == 'int64':\n",
    "            col_min, col_max = df[col].min(), df[col].max()\n",
    "            if col_min >= -128 and col_max <= 127:\n",
    "                df[col] = df[col].astype('int8')\n",
    "            elif col_min >= -32768 and col_max <= 32767:\n",
    "                df[col] = df[col].astype('int16')\n",
    "            else:\n",
    "                df[col] = df[col].astype('int32')\n",
    "        \n",
    "        # Floats\n",
    "        elif col_type == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        \n",
    "        # Objects to category\n",
    "        elif col_type == 'object' and df[col].nunique() / len(df) < 0.5:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply optimization\n",
    "orders = optimize_dtypes(orders, \"orders\")\n",
    "orders_prior = optimize_dtypes(orders_prior, \"orders_prior\")\n",
    "products = optimize_dtypes(products, \"products\")\n",
    "products_priced = optimize_dtypes(products_priced, \"products\")\n",
    "aisles = optimize_dtypes(aisles, \"aisles\")\n",
    "departments = optimize_dtypes(departments, \"departments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "041350cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/3] Saving optimized data...\n",
      "  All files saved to ../data/processed/\n",
      "\n",
      "Execution completed: 2026-02-20 14:59:28\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[3/3] Saving optimized data...\")\n",
    "\n",
    "# Save all files to processed folder\n",
    "orders.to_csv(f'{OUTPUT_DIR}/orders_optimized.csv', index=False)\n",
    "orders_prior.to_csv(f'{OUTPUT_DIR}/orders_prior_optimized.csv', index=False)\n",
    "products.to_csv(f'{OUTPUT_DIR}/products_optimized.csv', index=False)\n",
    "products_priced.to_csv(f'{OUTPUT_DIR}/products_priced_optimized.csv', index=False)\n",
    "aisles.to_csv(f'{OUTPUT_DIR}/aisles_optimized.csv', index=False)\n",
    "departments.to_csv(f'{OUTPUT_DIR}/departments_optimized.csv', index=False)\n",
    "\n",
    "print(\"  All files saved to ../data/processed/\")\n",
    "print(f\"\\nExecution completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81eb80f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "NOTEBOOK 01 COMPLETE\n",
      "============================================================\n",
      "\n",
      "Files created:\n",
      "  ✓ aisles_optimized.csv: 0.00 MB\n",
      "  ✓ aisle_performance.csv: 0.00 MB\n",
      "  ✓ baskets.csv: 245.92 MB\n",
      "  ✓ departments_optimized.csv: 0.00 MB\n",
      "  ✓ department_performance.csv: 0.00 MB\n",
      "  ✓ orders_optimized.csv: 106.59 MB\n",
      "  ✓ orders_prior_optimized.csv: 581.73 MB\n",
      "  ✓ products_optimized.csv: 2.11 MB\n",
      "  ✓ rfm_customer_segments.csv: 10.56 MB\n",
      "\n",
      "Next: Run Notebook 02 (Currency Conversion)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTEBOOK 01 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nFiles created:\")\n",
    "for file in os.listdir(OUTPUT_DIR):\n",
    "    if file.endswith('.csv'):\n",
    "        size_mb = os.path.getsize(f'{OUTPUT_DIR}/{file}') / 1024 / 1024\n",
    "        print(f\"  ✓ {file}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(f\"\\nNext: Run Notebook 02 (Currency Conversion)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
